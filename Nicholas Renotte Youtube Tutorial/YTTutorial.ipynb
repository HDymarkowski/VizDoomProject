{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable_baselines3\n",
      "  Downloading stable_baselines3-1.6.2-py3-none-any.whl (170 kB)\n",
      "Collecting gym==0.21\n",
      "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from stable_baselines3) (1.13.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (from stable_baselines3) (1.0.5)\n",
      "Collecting importlib-metadata~=4.13\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from stable_baselines3) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from stable_baselines3) (3.2.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\dell\\anaconda3\\lib\\site-packages (from stable_baselines3) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch>=1.11->stable_baselines3) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->stable_baselines3) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->stable_baselines3) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from importlib-metadata~=4.13->stable_baselines3) (3.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->stable_baselines3) (1.15.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616830 sha256=416d781ccfcc04c0dc055a82c4aa9cb3f0c285dd4bdd3c992575449dcdc7e032\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\27\\6d\\b3\\a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
      "Successfully built gym\n",
      "Installing collected packages: gym, importlib-metadata, stable-baselines3\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.26.2\n",
      "    Uninstalling gym-0.26.2:\n",
      "      Successfully uninstalled gym-0.26.2\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 5.2.0\n",
      "    Uninstalling importlib-metadata-5.2.0:\n",
      "      Successfully uninstalled importlib-metadata-5.2.0\n",
      "Successfully installed gym-0.21.0 importlib-metadata-4.13.0 stable-baselines3-1.6.2\n"
     ]
    }
   ],
   "source": [
    "! pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 : Converting to a Gym Enviroment\n",
    "\n",
    "from sre_parse import State\n",
    "from vizdoom import *\n",
    "from gym import Env # Import envoriment base class from OpenAI Gym\n",
    "from gym.spaces import Discrete, Box # Import gym spaces\n",
    "import cv2 # Import opencv to greyscale stuff\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Imports for part 3\n",
    "import torch # pyTorch\n",
    "import os # for file navigation\n",
    "from stable_baselines3.common.callbacks import BaseCallback # import callback class from stablebaselines 3\n",
    "from stable_baselines3.common import env_checker # For checking if enviroment is in correct format\n",
    "\n",
    "# Imports for Part 4\n",
    "from stable_baselines3 import PPO # Import PPO for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDOOM OpenAI Gym Enviroment\n",
    "class VizDoomGym(Env):\n",
    "\n",
    "    def __init__(self, render = False):\n",
    "\n",
    "\n",
    "        super().__init__() # Inherit from \"Env\" class ???\n",
    "        # TODO: Learn more about OpenAI Gym\n",
    "\n",
    "        # Set up game\n",
    "        self.game = vizdoom.DoomGame() # TODO: IDK if \"vizdoom.\" does anything\n",
    "        self.game.load_config('basic.cfg')\n",
    "\n",
    "       # Render frame logic\n",
    "       # Rendering takes a lot of computing and we don't always want to see shit cause we don't care\n",
    "        if(render == False):\n",
    "            self.game.set_window_visible(False) # Don't pop up that window - we don't want to see it\n",
    "        else:\n",
    "            self.game.set_window_visible(True) # Show us the window\n",
    "\n",
    "        # Start game after we know whether to render or not\n",
    "        self.game.init()\n",
    "\n",
    "        # Set up action space and observation space\n",
    "        # TODO: I don't really get what these are\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = (100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3) # 3 actions we can take\n",
    "\n",
    " \n",
    "\n",
    "    def step(self, action): # How we take a step in the enviroment\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3) # 3 actions, represented as [1,0,0], [0,1,0], [0,0,1]\n",
    "        reward = self.game.make_action(actions[action], 4) # Make the action adn get the reward, 4 = frameskip parameter\n",
    "\n",
    "        # Get other stuff we need to return\n",
    "        if (self.game.get_state()):\n",
    "            state = self.game.get_state().screen_buffer # The next frame of the game\n",
    "            state = self.greyscale(state) # Does the grayscaling and resizing of the image, implemented in greyscale() method\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo \n",
    "        else: # This logic in case we are finished and there is no next frame - would throw an error otherwise\n",
    "            # Just returns zeroes for shit\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "\n",
    "        done = self.game.is_episode_finished() # Whether or not the thing is finished\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def render(): # Predifined in Vizdoom but needed to be openAI superclass or smth\n",
    "        pass\n",
    "\n",
    "    def reset(self): # What happens when we start a new game\n",
    "        self.game.new_episode() # Make a new game\n",
    "        state = self.game.get_state().screen_buffer # Next frame\n",
    "\n",
    "        return self.greyscale(state) # Return next frame, greyscaled\n",
    "\n",
    "    def greyscale(self, observation): # Greyscale and resize the game frame, get rid of the bottom bit too\n",
    "        # Applied in step() and reset()\n",
    "        # Gets rid of color channel i.e. the 3\n",
    "        # TODO: Maybe figure out how this works\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY) # Making shit gray - idk how this works\n",
    "\n",
    "        # Compresses frame down ???\n",
    "        resize = cv2.resize(gray, (160, 100), interpolation = cv2.INTER_CUBIC) # Reiszes image and scales it down - so we have more pixels to process\n",
    "        state = np.reshape(resize, (100, 160, 1)) # \n",
    "\n",
    "        return state\n",
    "\n",
    "    def close(self): # Close down the game so it's not floating\n",
    "        self.game.close()\n",
    "\n",
    "# Enviroment is now set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTORIAL 3\n",
    "\n",
    "# Setup callback\n",
    "\n",
    "# Standard training and logging callback\n",
    "# Used for saving the model in case shit goes wrong\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose = 1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok = True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# Directories for saving and logging shit\n",
    "CHECKPOINT_DIR = './train/train_basic' # Checkpoint directory for saving trained reinforcement learning models\n",
    "LOG_DIR = './logs/log_basic' # \n",
    "\n",
    "# Create instance of train and logging callback\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "# check_freq = 10000 means that after every 10000 steps of training our model we're going to save a version of those pyTorch weights for our reinforcement learning agent (can be re-loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial 4 - Train the RL Model\n",
    "\n",
    "\n",
    "env = VizDoomGym(render = False) \n",
    "\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=256)\n",
    "# CnnPolicy as we are sending in an image\n",
    "# env passed through\n",
    "# LOG_DIR used for logging\n",
    "# Verbose means thaat we're going to have info appearing as we train\n",
    "# learning_rate can be increased\n",
    "# n_steps defines batch size for model. 256 = 256 sets of observations, actiosn, log probabililties and values will be stored in the buffer for one iteration\n",
    "#   > for basic is 300 so we use 256 (don't use whole max for game, just below)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
