{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ! installing stuff\n",
    "\n",
    "! jt -t oceans16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# PART 2 : Converting to a Gym Enviroment\n",
    "\n",
    "from sre_parse import State\n",
    "from vizdoom import *\n",
    "from gym import Env # Import envoriment base class from OpenAI Gym\n",
    "from gym.spaces import Discrete, Box # Import gym spaces\n",
    "import cv2 # Import opencv to greyscale stuff\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Imports for part 3\n",
    "import torch # pyTorch\n",
    "import os # for file navigation\n",
    "from stable_baselines3.common.callbacks import BaseCallback # import callback class from stablebaselines 3\n",
    "from stable_baselines3.common import env_checker # For checking if enviroment is in correct format\n",
    "import stable_baselines3\n",
    "\n",
    "# Imports for Part 4\n",
    "from stable_baselines3 import PPO # Import PPO for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDOOM OpenAI Gym Enviroment\n",
    "class VizDoomGym(Env):\n",
    "\n",
    "    def __init__(self, render = False, config='ViZDoom/scenarios/deadly_corridor_s1.cfg'):\n",
    "        super().__init__() # Inherit from \"Env\" class ???\n",
    "        # TODO: Learn more about OpenAI Gym\n",
    "\n",
    "        # Set up game\n",
    "        self.game = vizdoom.DoomGame() # TODO: IDK if \"vizdoom.\" does anything\n",
    "        self.game.load_config(config)\n",
    "\n",
    "       # Render frame logic\n",
    "       # Rendering takes a lot of computing and we don't always want to see shit cause we don't care\n",
    "        if(render == False):\n",
    "            self.game.set_window_visible(False) # Don't pop up that window - we don't want to see it\n",
    "        else:\n",
    "            self.game.set_window_visible(True) # Show us the window\n",
    "\n",
    "        # Start game after we know whether to render or not\n",
    "        self.game.init()\n",
    "\n",
    "        # Set up action space and observation space\n",
    "        # TODO: I don't really get what these are\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = (100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7) # 7 actions we can take now\n",
    "        \n",
    "        # Game variables are HEALTH, DAMAGE_TAKEN, HITCOUNT, SELECTED_WEAPON_AMMO\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0 \n",
    "        # Hitcount can be dodgy - it works with ALL monsters hit not even by our agent\n",
    "        # Damagecount can be better sometimes\n",
    "        # Look up ViZDoom game variables\n",
    "        self.ammo = 52\n",
    " \n",
    "\n",
    "    def step(self, action): # How we take a step in the enviroment\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(7) # 7 actions as in config file, represented as identity matrices\n",
    "        movement_reward = self.game.make_action(actions[action], 4) \n",
    "        # Make the action and get the reward, 4 = frameskip parameter\n",
    "        # TODO: I don't really get movement reward\n",
    "        \n",
    "        reward = 0\n",
    "        # Get other stuff we need to return\n",
    "        if (self.game.get_state()):\n",
    "            state = self.game.get_state().screen_buffer # The next frame of the game\n",
    "            state = self.greyscale(state) # Does the grayscaling and resizing of the image, implemented in greyscale() method\n",
    "            \n",
    "            # REWARD SHAPING\n",
    "            \n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            health, damage_taken, hitcount, ammo = game_variables\n",
    "            \n",
    "            # We want to calculate changes - DELTAS\n",
    "            damage_taken_delta = self.damage_taken - damage_taken\n",
    "            self.damage_taken = damage_taken\n",
    "            hitcount_delta = hitcount - self.hitcount # Remember that a hit kills\n",
    "            self.hitcount = hitcount\n",
    "            ammo_delta = ammo - self.ammo # You don't wannt to miss shots\n",
    "            self.ammo = ammo\n",
    "            \n",
    "            # The tutorial man says these weights work well - just taking his word for it\n",
    "            reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200 + ammo_delta*5\n",
    "            \n",
    "            # END OF REWARD SHAPING\n",
    "            \n",
    "            info = ammo \n",
    "        else: # This logic in case we are finished and there is no next frame - would throw an error otherwise\n",
    "            # Just returns zeroes for shit\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "\n",
    "        done = self.game.is_episode_finished() # Whether or not the thing is finished\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def render(): # Predifined in Vizdoom but needed to be openAI superclass or smth\n",
    "        pass\n",
    "\n",
    "    def reset(self): # What happens when we start a new game\n",
    "        self.game.new_episode() # Make a new game\n",
    "        state = self.game.get_state().screen_buffer # Next frame\n",
    "\n",
    "        return self.greyscale(state) # Return next frame, greyscaled\n",
    "\n",
    "    def greyscale(self, observation): # Greyscale and resize the game frame, get rid of the bottom bit too\n",
    "        # Applied in step() and reset()\n",
    "        # Gets rid of color channel i.e. the 3\n",
    "        # TODO: Maybe figure out how this works\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY) # Making shit gray - idk how this works\n",
    "\n",
    "        # Compresses frame down ???\n",
    "        resize = cv2.resize(gray, (160, 100), interpolation = cv2.INTER_CUBIC) # Reiszes image and scales it down - so we have more pixels to process\n",
    "        state = np.reshape(resize, (100, 160, 1)) # \n",
    "\n",
    "        return state\n",
    "\n",
    "    def close(self): # Close down the game so it's not floating\n",
    "        self.game.close()\n",
    "\n",
    "# Enviroment is now set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTORIAL 3\n",
    "\n",
    "# Setup callback\n",
    "\n",
    "# Standard training and logging callback\n",
    "# Used for saving the model in case shit goes wrong\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose = 1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok = True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# Directories for saving and logging shit\n",
    "CHECKPOINT_DIR = './train/train_corridor' # Checkpoint directory for saving trained reinforcement learning models\n",
    "LOG_DIR = './logs/log_corridor' # \n",
    "\n",
    "# Create instance of train and logging callback\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "# check_freq = 10000 means that after every 10000 steps of training our model we're going to save a version of those pyTorch weights for our reinforcement learning agent (can be re-loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Tutorial 4 - Train the RL Model usign curriculum learning\n",
    "\n",
    "\n",
    "env = VizDoomGym(render = False, config='ViZDoom/scenarios/deadly_corridor_s1.cfg')\n",
    "\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.00001, n_steps=8192, clip_range=0.10, gamma=0.95, gae_lambda=0.9)\n",
    "# Look at like 2:55 for epxlination of these hyperparameter values\n",
    "# n_steps = how many time frames I'm going to be passing in total as part of one traiing run\n",
    "# larger = more information we're going to be passing per epich\n",
    "# Should be a multiple (fuction) of 64 - 2:58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d6f6a329501c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Enviroment checker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv_checker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Runs with no errors if env is all good\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;31m# ============ Check the returned values ===============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     \u001b[0m_check_returned_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;31m# Sample a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"The `step()` method must return four values: obs, reward, done, info\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9e1fbd3752ab>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mgame_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mhealth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdamage_taken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhitcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mammo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m# We want to calculate changes - DELTAS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Enviroment checker\n",
    "\n",
    "env_checker.check_env(env) # Runs with no errors if env is all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the PPO training model\n",
    "\n",
    "model.learn(total_timesteps=100_000, callback=callback )\n",
    "\n",
    "# Get tensorboard graphs with (in ppo training folder) \"tensorboard --logrid=.\"\n",
    "# He explains tensorboard graphs at 1:42\n",
    "\n",
    "# Agent is now trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial 5 - Testing the Agent\n",
    "import time\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('./train/train_defend/best_model_70000') # Reload model from disk\n",
    "##PPO.load() works too\n",
    "\n",
    "env = VizDoomGym(render = True)\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes = 10)\n",
    "# \"_\" is used when something you don't care about is returned\n",
    "# pass in model, enviroment and amount of episodes we want to evaluate the policy for\n",
    "\n",
    "print(mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a lot like what we did in the very beginning i.e. the initial tutorial\n",
    "\n",
    "for episode in range(5): # 5 games\n",
    "    obs = env.reset() # reset enviroment, store game frame in \"observations\" variable\n",
    "    done = False\n",
    "    total_reward= 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs) # model predicts what ation to take\n",
    "        obs, reward, done, info = env.step(action) # actually take the predicted action\n",
    "        time.sleep(0.25)\n",
    "        total_reward += reward\n",
    "    print(\"Total reward for episode {} is {}\".format(episode, total_reward))\n",
    "    time.sleep(2)\n",
    "    \n",
    "# 1:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial 7 - Deadly Corridor\n",
    "\n",
    "\"\"\"\n",
    "Stuff you can do to train agents to play harder games, 2 techniques usually work pretty well\n",
    "    > Reward shaping - helps ease the sparse reward problem\n",
    "        - So like if you're doing a maze and the only reward is finishing the maze, the agent\n",
    "          gets fuck all info from that\n",
    "        - Reward shaping gives incrimental feedback and incrimental rewards for completing \n",
    "          sub-goals i.e. getting closer to the end of the maze rather than just completing it\n",
    "    > Curriculum learning - train the agent in easier enviroment and progressively increase \n",
    "      the complexity and/or difficulty so the agent learns to play in a real-world scenario\n",
    "          - For Doom, we can do this by changing the difficulty setting int the config file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Look at \"CLIENT CONVERSATION 6\" for an explination of this\n",
    "\n",
    "# First we do reward shaping, adding new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our Model using Curriculum Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(game.get_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: LOOK INTO OPTUNA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
