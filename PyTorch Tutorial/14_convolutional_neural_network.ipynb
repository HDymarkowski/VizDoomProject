{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- Pooling layers -\\n\\nIn this case we are talking about max pooling\\nMax pooling is used to downsample an image by applying a \"max\" filter to sub-regions\\nThis reduces the computational cost by reducing the size of the image\\n    > reducing the number of parmeters our model has to learn\\n    > Also helps to avoid \"overfitting\" by providing an abstracted form of the input\\n\\nNOTE: Downsample = to make smaller by making smaller sampling rate (look at image at 3:17)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We will be implementig a CNN (Convolutional neural network) and doing image classification based\n",
    "on the CIFAR-10 dataset (dataset with 10 differen classes like plane, bird, dog, truck, ect.)\n",
    "This dataset is also available directly in PyTorch\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "CNNs are similar to normal neural networks:\n",
    "    > They are mad eup of neurons that have learnable wieghts and biases\n",
    "    > The main difference is that CNNs mainly work on image data and apply the so called\n",
    "      \"convolutional\" filters\n",
    "      \n",
    "A typical CNN architecture has:\n",
    "    > Image\n",
    "    > Different convolutional layers and activation functions followed by \"pooling\" layers\n",
    "        - These pooling layers are used to automatically learn some features from the images\n",
    "    > Then at the end we have one or more \"FC\" (fully connected) layers for the actual classification\n",
    "      task\n",
    "      \n",
    "Convolutional filters work by applying filter kernel to our image (remember the \"What is convolution\" \n",
    "video):\n",
    "    > Put the filter at first position\n",
    "    > Compute output value by multiplying and summing up all the values\n",
    "    > Write value into output image\n",
    "    > Slide our filter to next position\n",
    "    > Then do same thing with same filter operations, just keep sliding over whole image\n",
    "    \n",
    "With this tranform, our resulting image may have a smaller size as our filter does not fit in the \n",
    "corners or something, except if we use a technique called padding (which we will not cover in this\n",
    "lecture)\n",
    "    > So getting the correct size is an important step\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "- Pooling layers -\n",
    "\n",
    "In this case we are talking about max pooling\n",
    "Max pooling is used to downsample an image by applying a \"max\" filter to sub-regions\n",
    "This reduces the computational cost by reducing the size of the image\n",
    "    > reducing the number of parmeters our model has to learn\n",
    "    > Also helps to avoid \"overfitting\" by providing an abstracted form of the input\n",
    "\n",
    "NOTE: Downsample = to make smaller by making smaller sampling rate (look at image at 3:17)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# The dataset has PILImages of range [0,1]\n",
    "# PIL = pillow = a python image library\n",
    "# We transform them to tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Get datasets\n",
    "# CIFAR10 dataset in PyTorch\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "# Loaders for automatic batch optimization and batch training\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False) # Don't need to shuffle test data\n",
    "\n",
    "# Classes hardcoded\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement convolutional neural netowrk\n",
    "# The imporant part for this tutorial\n",
    "\n",
    "\"\"\"\n",
    "The layout:\n",
    "    > Input\n",
    "    > Feature learning:\n",
    "        - Convolution layer + ReLu activation functin\n",
    "        - Max pooling\n",
    "        - Repeat the above 2 (x times?), 2 times in this example\n",
    "    > Classification:\n",
    "        - Flatten\n",
    "        - Fully connected\n",
    "        - Softmax (included in cross-entropy)\n",
    "\"\"\"\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ConvNet, self).__init__() # This is needed for a conv net\n",
    "        \n",
    "        # Define layers\n",
    "        # We do the activation functions in the forward pass, different to before\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # Parameters are (input channel size, output channel size, kernel size)\n",
    "        # Input cannel size is 3 as out images have 3 color channels\n",
    "        # Output channel size is 6\n",
    "        # Kernel size is 5 (5x5)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Parameters are (kernel size, stride)\n",
    "        # Kernel size being how big the pool is i.e. in this case 2x2\n",
    "        # Stride being by how much we shift\n",
    "        # ^^^ I think??\n",
    "        # Exactly as image in tutorial\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Input channel size must be equal to last output channel size\n",
    "        # Ouput size is 16\n",
    "        # Kernel size is still 5\n",
    "        \n",
    "        # Now set up fully connected layers\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        \"\"\"\n",
    "        Input size isfound by ((input width - filter size + 2 * padding) / stride) + 1\n",
    "            > Read as (input size x input size) I think\n",
    "        This is found in the above example to be 16*5*5, hence this input size\n",
    "            > torch.size(4 x 16 x 5 x 5) so 16 x 5 x 5 for each sample\n",
    "            > Flattening each sample gives us 16*5*5\n",
    "        \"\"\"\n",
    "        # Output size is 120 but can be played with\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        # 10 different classes so final output size must be 10\n",
    "        \n",
    "        \"\"\"\n",
    "        The 120 and the 84 here are intermediat and so can be changed / playd about with\n",
    "        However, the 16*5*5 and the 10 are fixed and cannot be changed\n",
    "        \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First convolutional and pooling layer\n",
    "        x = self.pool(F.relu(self.conv1(x))) # ReLU activation function applied here\n",
    "        # NOTE: Activation function does not change the size\n",
    "        \n",
    "        # Second convolutional and pooling layer\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Need to flatten it to pass to first fully connected layer\n",
    "        x = x.view(-1, 16*5*5) # -1 = number of samples in  batch\n",
    "        \n",
    "        # First fully connected layer\n",
    "        x = F.relu(self.fc1(x)) # Activation function called again\n",
    "        \n",
    "        # Second fully conneted layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        # Notice no activation function at the end and no softmax as it is already included in loss\n",
    "        # ... (criterion)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "\n",
    "# Loss function (criterion)\n",
    "# CrossEntropyLoss() used as it is a multi-class classification problem\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Optimizer\n",
    "# Parameters are model parameters and learning rate\n",
    "# Stochastic graient descent used to optimize model parameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [2000/12500, Loss: 2.2472\n",
      "Epoch [1/4], Step [4000/12500, Loss: 2.2879\n",
      "Epoch [1/4], Step [6000/12500, Loss: 2.3134\n",
      "Epoch [1/4], Step [8000/12500, Loss: 2.2661\n",
      "Epoch [1/4], Step [10000/12500, Loss: 2.4327\n",
      "Epoch [1/4], Step [12000/12500, Loss: 1.7628\n",
      "Epoch [2/4], Step [2000/12500, Loss: 2.6596\n",
      "Epoch [2/4], Step [4000/12500, Loss: 2.0702\n",
      "Epoch [2/4], Step [6000/12500, Loss: 2.3234\n",
      "Epoch [2/4], Step [8000/12500, Loss: 1.6211\n",
      "Epoch [2/4], Step [10000/12500, Loss: 1.9212\n",
      "Epoch [2/4], Step [12000/12500, Loss: 1.9476\n",
      "Epoch [3/4], Step [2000/12500, Loss: 1.7850\n",
      "Epoch [3/4], Step [4000/12500, Loss: 2.8360\n",
      "Epoch [3/4], Step [6000/12500, Loss: 2.4755\n",
      "Epoch [3/4], Step [8000/12500, Loss: 1.9289\n",
      "Epoch [3/4], Step [10000/12500, Loss: 1.8415\n",
      "Epoch [3/4], Step [12000/12500, Loss: 0.6593\n",
      "Epoch [4/4], Step [2000/12500, Loss: 1.5006\n",
      "Epoch [4/4], Step [4000/12500, Loss: 1.5051\n",
      "Epoch [4/4], Step [6000/12500, Loss: 1.5913\n",
      "Epoch [4/4], Step [8000/12500, Loss: 1.2349\n",
      "Epoch [4/4], Step [10000/12500, Loss: 1.5851\n",
      "Epoch [4/4], Step [12000/12500, Loss: 1.0429\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "n_total_steps = len(train_loader) # 12_500\n",
    "\n",
    "for epoch in range(num_epochs): # Loop over epochs\n",
    "    for i, (images, labels) in enumerate(train_loader): # Loop over batches\n",
    "        # Original shape = [4, 3, 32, 32]\n",
    "        # 4 in each batch\n",
    "        # 3 color channels\n",
    "        # 32 x 32 image (32 * 32 = 1024)\n",
    "        # Input layer has 3 input channels, 6 output channels and 5 kernel size\n",
    "        \n",
    "        # Pass images and labels to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        ## Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        ## Backwards pass\n",
    "        optimizer.zero_grad() # Empty the gradients\n",
    "        loss.backward() # Backpropogation\n",
    "        \n",
    "        ## Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## Print data\n",
    "        \n",
    "        if ((i+1) % 2000 == 0):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 46.23 %\n",
      "Accuracy of plane: 53.6 %\n",
      "Accuracy of car: 50.4 %\n",
      "Accuracy of bird: 32.9 %\n",
      "Accuracy of cat: 18.3 %\n",
      "Accuracy of deer: 32.1 %\n",
      "Accuracy of dog: 44.1 %\n",
      "Accuracy of frog: 60.4 %\n",
      "Accuracy of horse: 67.0 %\n",
      "Accuracy of ship: 57.49999999999999 %\n",
      "Accuracy of truck: 46.0 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Max returns (value, index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "            \n",
    "        accuracy = 100.0 * (n_correct / n_samples)\n",
    "    \n",
    "    print(f'Accuracy of the network: {accuracy} %')\n",
    "        \n",
    "    for i in range(10):\n",
    "        accuracy = 100.0 * (n_class_correct[i] / n_class_samples[i])\n",
    "        print(f'Accuracy of {classes[i]}: {accuracy} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Network sucks because too few epochs, try more epochs or maybe change other hyperparameters\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
