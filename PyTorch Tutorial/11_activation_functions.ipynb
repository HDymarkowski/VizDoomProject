{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActivation functions are a very important feature of neural networks\\n\\nActivation functions apply a non-linear transform and decide whether a neuron should be activate \\nor not\\n\\nWithout activation functions our network is basically jsut a stacked linear regression model, which\\nis not suitable for more complex tasks\\n\\nWith non-linear trasformations our network can learn better and perform more complex tasks\\nAfter each layer we typically use an activation layer\\n\\nThe most popular activation functions:\\n    1. Step function\\n    2. Sigmoid\\n    3. Tanh\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Activation functions are a very important feature of neural networks\n",
    "\n",
    "Activation functions apply a non-linear transform and decide whether a neuron should be activate \n",
    "or not\n",
    "\n",
    "Without activation functions our network is basically jsut a stacked linear regression model, which\n",
    "is not suitable for more complex tasks\n",
    "\n",
    "With non-linear trasformations our network can learn better and perform more complex tasks\n",
    "After each layer we typically use an activation layer\n",
    "\n",
    "The most popular activation functions (Watch like 2:43 onwards for examples):\n",
    "    1. Step function\n",
    "        > 1 if x > 0\n",
    "        > 0 otherwise\n",
    "        > Not usd in practise\n",
    "    2. Sigmoid\n",
    "        > Typically the last layer of a binary classification problem\n",
    "    3. TanH\n",
    "        > Hyperbolic tangent function\n",
    "        > Basically a scaled and shifted sigmoid function\n",
    "        > A good choice in hidden layers\n",
    "    4. ReLU\n",
    "        > Most popular choice in most of the networks\n",
    "        > f(x) = max(0, x)\n",
    "            - 0 for -ve, x for x > 0\n",
    "        > Rule of thumb: If you don't know what to use, just sue ReLU for hidden layers\n",
    "        > Potential vanishing gradient problem, gradient for -ve is 0 which messes with the\n",
    "          back propogation, -ve neurons are \"dead\" (-ve values will not learn anything or whatever)\n",
    "    5. Leaky ReLU\n",
    "        > Improved version of ReLU, tries to solve hte vanishing gradient problem\n",
    "        > If -ve, a * x, x otherwise (a is very small i.e. 0.001)\n",
    "    6. Softmax\n",
    "        > Gives you probability as output - squashes values to be between 0 and 1 and add up to 1\n",
    "        > Good in last layer in multi class classification problems\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Look at the bottom of the page\n",
    "\n",
    "# We have 2 options..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 - create nn modules\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super (NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) # Input, output\n",
    "        self.relu = nn.ReLU() # ReLU activation function\n",
    "        self.linear2 = nn.Linear(hidden_size, 1) # Input, output\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass just goes through all of the layers\n",
    "        out = self.linear1(x) # First input is x\n",
    "        out = self.relu(out) # Then we just pass about out\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 - use activation functions direcrly in forward pass\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear (input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x)) # torch.relu used here isntead\n",
    "        out = torch.sigomid(self.linear2(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Both ways achive the same ting, it's just a matter of how you prefer your code\n",
    "\n",
    "Some of the functions aren't available in the torch API directly, so you need to use, for example,\n",
    "F.leaky_relu() (torch.nn.functional.leaky_relu())\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
