{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will continue where we left off, we will now replace the manually computed loss and parameter\n",
    "updates using the loss and optimizr classes in PyTorch\n",
    "Then we will alos replace the manually computed model prediction by implementing a PyTorch model\n",
    "Then, PyTorch can do the complete pipeline for us \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) =  0.000\n",
      "Epoch 1: w = 0.300, loss = 30.00000000\n",
      "Epoch 11: w = 1.665, loss = 1.16278565\n",
      "Epoch 21: w = 1.934, loss = 0.04506890\n",
      "Epoch 31: w = 1.987, loss = 0.00174685\n",
      "Epoch 41: w = 1.997, loss = 0.00006770\n",
      "Epoch 51: w = 1.999, loss = 0.00000262\n",
      "Epoch 61: w = 2.000, loss = 0.00000010\n",
      "Epoch 71: w = 2.000, loss = 0.00000000\n",
      "Epoch 81: w = 2.000, loss = 0.00000000\n",
      "Epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) =  10.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIt ends up not as good as the one above becaus the back propogation is not as exact as the numerical\\ngradient compuation\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we are doing the things detailed above\n",
    "\n",
    "\"\"\"\n",
    "General training pipeline in PyTorch:\n",
    "    1. Design model (input_size, output_size, foward_pass(operations / layers))\n",
    "    2. Constructloss and optimizer\n",
    "    3. Training loop:\n",
    "        - Forward pass: compute prediction\n",
    "        - Backward pass: gradients\n",
    "        - Update weights \n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn # Neural Network module\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x (w=2)\n",
    "# We are taking the real value of W to be 2???\n",
    "\n",
    "# Training samples\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32) # Training input\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32) # 2 * x, Training output\n",
    "\n",
    "# w also has to be a tensor in this model\n",
    "# Since we are interested in the gradient of our loss with respect ot w, we need to specify\n",
    "# ... that w requires the gradient calculattionn\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad = True) # Initially 0\n",
    "\n",
    "# Model Prediciton\n",
    "def forward(x):\n",
    "    # Forward pass to follow the conventions of pytorch\n",
    "    return w * x\n",
    "\n",
    "# We are no longer manually calculating loss\n",
    "\n",
    "# Remove manually computed gradient\n",
    "\n",
    "# Main program\n",
    "print(f'Prediction before training: f(5) =  {forward(5):.3f}') # f(5) should be 10\n",
    "\n",
    "# Training\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 100 # Number of iteration\n",
    "\n",
    "loss = nn.MSELoss() # implemented in PyTorch, MSELoss = mean squared error loss\n",
    "# Exactly what we manually implpemeneted before\n",
    "\n",
    "optimizer = torch.optim.SGD([w], lr = learning_rate) \n",
    "# SGD = stochasitc gadient descent\n",
    "# Parameters are the weights and the learning rage\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediciton = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # Loss\n",
    "    l = loss(Y, y_pred) # Now calsl nn.MSELoss()\n",
    "    # Could be written as l = nn.MSELoss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass (remember: back propogation)\n",
    "    l.backward() # Calculates dLoss/dWeights (dl/dw)\n",
    "    # PyTorch does all the calculations for us\n",
    "    \n",
    "    # Update weights\n",
    "    # We no longer need to do this manually\n",
    "    optimizer.step()\n",
    "    \n",
    "    \"\"\"\n",
    "    We also need to zero our gradients, because otherwise l.backward() will keep accumulating\n",
    "    the w.grad attribute\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad() # We still have to empty our graditns though\n",
    "    \n",
    "    if(epoch % 10 == 0):\n",
    "        print(f'Epoch {epoch  + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "print(f'Prediction after training: f(5) =  {forward(5):.3f}')\n",
    "\"\"\"\n",
    "It ends up not as good as the one above becaus the back propogation is not as exact as the numerical\n",
    "gradient compuation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) =  -4.271\n",
      "Epoch 1: w = -0.314, loss = 64.07299805\n",
      "Epoch 11: w = 1.522, loss = 1.68022776\n",
      "Epoch 21: w = 1.821, loss = 0.06466764\n",
      "Epoch 31: w = 1.872, loss = 0.02163561\n",
      "Epoch 41: w = 1.883, loss = 0.01936034\n",
      "Epoch 51: w = 1.888, loss = 0.01820717\n",
      "Epoch 61: w = 1.891, loss = 0.01714674\n",
      "Epoch 71: w = 1.895, loss = 0.01614869\n",
      "Epoch 81: w = 1.898, loss = 0.01520874\n",
      "Epoch 91: w = 1.901, loss = 0.01432352\n",
      "Prediction after training: f(5) =  9.801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIt ends up not as good as the one above becaus the back propogation is not as exact as the numerical\\ngradient compuation\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last step is to replace the forward() method with a PyTorch model\n",
    "\n",
    "import torch.nn as nn # Neural Network module\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x (w=2)\n",
    "# We are taking the real value of W to be 2???\n",
    "\n",
    "# Training samples\n",
    "\n",
    "\"\"\"\n",
    "We need to change this for the model\n",
    "Needs to be a 2d array where the number of rows is the number of samples and for each row we have\n",
    "the features\n",
    "\"\"\"\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # Training input\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) # 2 * x, Training output\n",
    "\n",
    "n_samples, n_features = X.shape \n",
    "# n_samples (amount of inputs) = 4\n",
    "# n_features (amount of numbers in each input) = 1\n",
    "#     - So like if instead of [1] it was [1,2,3] the n_features would be 3\n",
    "\n",
    "# Weights removed form here\n",
    "\n",
    "# Model Prediciton\n",
    "# Now we do this via PyTorch\n",
    "# Linear is very trivial - only one layer\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# BELOW IS FOR THE DEFAULT ONE, WE CREATE A CUSTOM ONE BELOW THAT\n",
    "# model = nn.Linear(input_size, output_size) # Parameters are input size and output size of features\n",
    "\n",
    "# Say we need a custom model\n",
    "# We do it here:\n",
    "class LinearRegression(nn.Module):\n",
    "    # Custom linear regression model\n",
    "    # This example actually does exactly the same as the normal linear regression model\n",
    "    # ... but just here as an example of how we can edit stuff\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__() # This is how we call the \"super\" constuctor\n",
    "        # ... Inherits from linear regression\n",
    "        \n",
    "        # Define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim) # Linear layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# We are no longer manually calculating loss\n",
    "\n",
    "# Remove manually computed gradient\n",
    "\n",
    "# Main program\n",
    "# Look at how this is called now, instead of forwards()\n",
    "X_test = torch.tensor([5], dtype=torch.float32) # Test tensor - needed for testing\n",
    "print(f'Prediction before training: f(5) =  {model(X_test).item():.3f}') # f(5) should be 10\n",
    "# .item() called since output is only of size 1, we want the actual value\n",
    "\n",
    "# Training\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 100 # Number of iteration\n",
    "\n",
    "loss = nn.MSELoss() # implemented in PyTorch, MSELoss = mean squared error loss\n",
    "# Exactly what we manually implpemeneted before\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # Weights is now model.parameters()\n",
    "# SGD = stochasitc gadient descent\n",
    "# Parameters are the weights and the learning rage\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediciton = forward pass\n",
    "    y_pred = model(X) # Prediction now just calls model\n",
    "    \n",
    "    # Loss\n",
    "    l = loss(Y, y_pred) # Now calsl nn.MSELoss()\n",
    "    # Could be written as l = nn.MSELoss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass (remember: back propogation)\n",
    "    l.backward() # Calculates dLoss/dWeights (dl/dw)\n",
    "    # PyTorch does all the calculations for us\n",
    "    \n",
    "    # Update weights\n",
    "    # We no longer need to do this manually\n",
    "    optimizer.step()\n",
    "    \n",
    "    \"\"\"\n",
    "    We also need to zero our gradients, because otherwise l.backward() will keep accumulating\n",
    "    the w.grad attribute\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad() # We still have to empty our graditns though\n",
    "    \n",
    "    if(epoch % 10 == 0):\n",
    "        [w, b]  = model.parameters() # Need to unpack them to print them now\n",
    "        # w = weights\n",
    "        # b = bias ???\n",
    "        # These are like lists of lists so we need to get it like w below\n",
    "        print(f'Epoch {epoch  + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "    \n",
    "print(f'Prediction after training: f(5) =  {model(X_test).item():.3f}') # f(5) should be 10\n",
    "\"\"\"\n",
    "It ends up not as good as the one above becaus the back propogation is not as exact as the numerical\n",
    "gradient compuation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
