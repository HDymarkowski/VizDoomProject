{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Softmax and cross-entropy are two of the most common functions used in neural networks so we should\n",
    "know how they work\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "-- Softmax --\n",
    "\n",
    "Squashes the ouptut to be between 0 and 1 so we get probabilities that add up to 1\n",
    "Allows predictions to be made (one with highest probability chosen)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax numpy:  [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "# Softmax in numpy\n",
    "\n",
    "def softmax(x):\n",
    "    # Below is the formula\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0) # Tell it the dimension on which to calculate\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print(\"Softmax numpy: \", outputs ) # Softmax numpy:  [0.65900114 0.24243297 0.09856589]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax torch:  tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "# Softmax in PyTorch\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0) # Still tell it the dimension\n",
    "\n",
    "print(\"Softmax torch: \", outputs ) # Softmax torch:  tensor([0.6590, 0.2424, 0.0986])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Softmax is often combined with the soft-entropy loss\n",
    "Cross-entropy  is a type of loss that measures the performance of our classification model whose \n",
    "output is a probability between 0 and 1, it can be used in multi-class problems and the loss increases\n",
    "as the predicted probability diverges from the actual label\n",
    "\n",
    "The better our prediciton, the lower our losss\n",
    "    > y = real, y_hat = prediction probabilities\n",
    "    > y has to be one-hot encoded meaning one 1 and the rest are 0 (only one correct output)\n",
    "    > y_hat are the probabilities\n",
    "    > y = [1,0,0], y_hat = [.7, .2, .1] has cross-entropy 0.35 which is good\n",
    "    > y = [1,0,0], y_hat = [.1, .3, .6] has cross-entropy of 2.30 which is bad\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n"
     ]
    }
   ],
   "source": [
    "# Cross-entropy loss in numpy\n",
    "\n",
    "def cross_entropy(actual, predicted):\n",
    "    # The following is the cross-entropy loss formula\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0]) To normalise it\n",
    "\n",
    "\"\"\"\n",
    "y must be one hot encoded\n",
    "If class = 0 [1 0 0]\n",
    "If class = 1 [0 1 0]\n",
    "If class = 2 [0 0 1]\n",
    "\"\"\"\n",
    "\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "# y_pred (y_hat) has probabilities\n",
    "\n",
    "Y_pred_good = np.array([.7, .2, .1])\n",
    "Y_pred_bad = np.array([.1, .3, .6])\n",
    "\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "\n",
    "print(f'Loss1 numpy: {l1:.4f}') # 0.3567\n",
    "print(f'Loss2 numpy: {l2:.4f}') # 2.3026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 torch: 0.4170\n",
      "Loss2 torch: 1.8406\n",
      "prediciton 1: tensor([0])\n",
      "prediciton 2: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Cross-entropy in PyTorch\n",
    "\n",
    "\"\"\"\n",
    "Careful!\n",
    "nn.CrossEntropyLoss() applies nn.LogSoftmax + nn.NLLLoss (negative likelihood loss)\n",
    "    > No Softmax in last layer!\n",
    "    > Y has class labels, not One-Hot!\n",
    "    > Y_pred (y_hat) has raw scores (logits), no Softmax\n",
    "        - NOTE : Logit = probability which, instead of between 0 and 1, is between -inf and inf\n",
    "            > It is the inverse to the sigmoid function, limits values between 0 and one on Y-axis\n",
    "              rather than X-axis\n",
    "\"\"\"\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([0]) # Class 0 is the correct class, equivalent to [1 0 0] in the above example\n",
    "\n",
    "# Y_pred has size n_samples * n_classes\n",
    "# ... in this case, 1 x 3\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(f'Loss1 torch: {l1.item():.4f}') # 0.4170\n",
    "print(f'Loss2 torch: {l2.item():.4f}') # 1.8406\n",
    "\n",
    "# To get the actual predictions\n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "\n",
    "print(f'prediciton 1: {predictions1}') # tensor[0]\n",
    "print(f'prediciton 2: {predictions2}') # tensor[1]\n",
    "\n",
    "# Meaning that index 0 is the highest for the first one, index 2 is the hihest for the second one\n",
    "# ... which is the case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 torch: 0.3018\n",
      "Loss2 torch: 1.6242\n",
      "prediciton 1: tensor([2, 0, 1])\n",
      "prediciton 2: tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# The loss in PyTorch allows for multiple samples\n",
    "# We can increase samples to 3\n",
    "# Listen to like 2:35 for what this means, it makes sense\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3 samples\n",
    "Y = torch.tensor([2, 0, 1]) # Requires 3 class labels\n",
    "\n",
    "# Y_pred has size n_samples * n_classes\n",
    "# ... in this case, 3 x 3\n",
    "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1], [2.0, 1.0, 0.1], [0.1, 3.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [0.1, 3.0, 0.1]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(f'Loss1 torch: {l1.item():.4f}')\n",
    "print(f'Loss2 torch: {l2.item():.4f}')\n",
    "\n",
    "# To get the actual predictions\n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "\n",
    "print(f'prediciton 1: {predictions1}') # tensor[0]\n",
    "print(f'prediciton 2: {predictions2}') # tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For multiclass problems (\"which animal is the pictur?\") in PyTorch, use nn.CrossEntropyLoss()\n",
    "    > No Softmax at the end!\n",
    "For binary problesm (\"Is the picture a dog?\") in PyTorch, use nn.BCELoss()\n",
    "    > Sigomid function at the end\n",
    "        - > .5 = yes\n",
    "        - < .5 = no\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
