{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we will see how we optimise our model using automatic gradient computation, using the PyTorch\n",
    "autograd package\n",
    "First we will implement the linear regression model form scratch (doing every step manually)\n",
    "    > Prediciton - manually\n",
    "    > Gradient computation - manually\n",
    "    > Loss computation - manually\n",
    "    > Parameter updates - manually\n",
    "    \n",
    "Then, we will see how we can do this using PyTorch\n",
    "    > Prediciton - PyTorch Model\n",
    "    > Gradients computation - Autograd\n",
    "    > Loss computation - PyTorch Loss\n",
    "    > Parameter updates - PyTorch Optimizer\n",
    "    \n",
    "So basically, PyTorch can do most of our work for us, but we still need to know what loss and \n",
    "omptimzer to use\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this video, we will do the manual prediction and the autograd gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) =  0.000\n",
      "Epoch 1: w = 1.200, loss = 30.00000000\n",
      "Epoch 3: w = 1.872, loss = 0.76800019\n",
      "Epoch 5: w = 1.980, loss = 0.01966083\n",
      "Epoch 7: w = 1.997, loss = 0.00050331\n",
      "Epoch 9: w = 1.999, loss = 0.00001288\n",
      "Epoch 11: w = 2.000, loss = 0.00000033\n",
      "Epoch 13: w = 2.000, loss = 0.00000001\n",
      "Epoch 15: w = 2.000, loss = 0.00000000\n",
      "Epoch 17: w = 2.000, loss = 0.00000000\n",
      "Epoch 19: w = 2.000, loss = 0.00000000\n",
      "Epoch 21: w = 2.000, loss = 0.00000000\n",
      "Epoch 23: w = 2.000, loss = 0.00000000\n",
      "Epoch 25: w = 2.000, loss = 0.00000000\n",
      "Epoch 27: w = 2.000, loss = 0.00000000\n",
      "Epoch 29: w = 2.000, loss = 0.00000000\n",
      "Epoch 31: w = 2.000, loss = 0.00000000\n",
      "Epoch 33: w = 2.000, loss = 0.00000000\n",
      "Epoch 35: w = 2.000, loss = 0.00000000\n",
      "Epoch 37: w = 2.000, loss = 0.00000000\n",
      "Epoch 39: w = 2.000, loss = 0.00000000\n",
      "Epoch 41: w = 2.000, loss = 0.00000000\n",
      "Epoch 43: w = 2.000, loss = 0.00000000\n",
      "Epoch 45: w = 2.000, loss = 0.00000000\n",
      "Epoch 47: w = 2.000, loss = 0.00000000\n",
      "Epoch 49: w = 2.000, loss = 0.00000000\n",
      "Epoch 51: w = 2.000, loss = 0.00000000\n",
      "Epoch 53: w = 2.000, loss = 0.00000000\n",
      "Epoch 55: w = 2.000, loss = 0.00000000\n",
      "Epoch 57: w = 2.000, loss = 0.00000000\n",
      "Epoch 59: w = 2.000, loss = 0.00000000\n",
      "Epoch 61: w = 2.000, loss = 0.00000000\n",
      "Epoch 63: w = 2.000, loss = 0.00000000\n",
      "Epoch 65: w = 2.000, loss = 0.00000000\n",
      "Epoch 67: w = 2.000, loss = 0.00000000\n",
      "Epoch 69: w = 2.000, loss = 0.00000000\n",
      "Epoch 71: w = 2.000, loss = 0.00000000\n",
      "Epoch 73: w = 2.000, loss = 0.00000000\n",
      "Epoch 75: w = 2.000, loss = 0.00000000\n",
      "Epoch 77: w = 2.000, loss = 0.00000000\n",
      "Epoch 79: w = 2.000, loss = 0.00000000\n",
      "Epoch 81: w = 2.000, loss = 0.00000000\n",
      "Epoch 83: w = 2.000, loss = 0.00000000\n",
      "Epoch 85: w = 2.000, loss = 0.00000000\n",
      "Epoch 87: w = 2.000, loss = 0.00000000\n",
      "Epoch 89: w = 2.000, loss = 0.00000000\n",
      "Epoch 91: w = 2.000, loss = 0.00000000\n",
      "Epoch 93: w = 2.000, loss = 0.00000000\n",
      "Epoch 95: w = 2.000, loss = 0.00000000\n",
      "Epoch 97: w = 2.000, loss = 0.00000000\n",
      "Epoch 99: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) =  10.000\n"
     ]
    }
   ],
   "source": [
    "# All Done manually first\n",
    "\n",
    "# f = w =* x\n",
    "# f = 2 * x (w=2)\n",
    "# We are taking the real value of W to be 2???\n",
    "\n",
    "# Training samples\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32) # Training input\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32) # 2 * x, Training output\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# Model Prediciton\n",
    "def forward(x):\n",
    "    # Forward pass to follow the conventions of pytorch\n",
    "    return w * x\n",
    "\n",
    "# Loss = mean squared error\n",
    "def loss(y, y_predicted):\n",
    "    # y_predicted = model output\n",
    "    return (((y_predicted - y)**2).mean())\n",
    "\n",
    "# Gradient - calcualate gradient of the loss with respect to our parameters\n",
    "# MSE (mean squared error) = 1 / N * ((w * x)  - y)**2\n",
    "# = 1/N * (prediction - actual value) squared\n",
    "# Derivative of above = dJ/dw = 1/N * 2x * (w*x - y)\n",
    "# J = objecitve function\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    # np.dot = top product\n",
    "    # Below is the derivative formula shown above\n",
    "    return np.dot( 2 * x, y_predicted - y).mean()\n",
    "\n",
    "\n",
    "# Main program\n",
    "print(f'Prediction before training: f(5) =  {forward(5):.3f}') # f(5) should be 10\n",
    "\n",
    "# Training\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20 # Number of iteration\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediciton = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # Update weights\n",
    "    w -= learning_rate * dw\n",
    "    # This is the update formula for gradient descent\n",
    "    # Our weight, W, goes in the negative direction of the learning rate * the gradient\n",
    "    \n",
    "    if(epoch % 2 == 0):\n",
    "        print(f'Epoch {epoch  + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "print(f'Prediction after training: f(5) =  {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) =  0.000\n",
      "Epoch 1: w = 0.300, loss = 30.00000000\n",
      "Epoch 3: w = 0.772, loss = 15.66018772\n",
      "Epoch 5: w = 1.113, loss = 8.17471695\n",
      "Epoch 7: w = 1.359, loss = 4.26725292\n",
      "Epoch 9: w = 1.537, loss = 2.22753215\n",
      "Epoch 11: w = 1.665, loss = 1.16278565\n",
      "Epoch 13: w = 1.758, loss = 0.60698116\n",
      "Epoch 15: w = 1.825, loss = 0.31684780\n",
      "Epoch 17: w = 1.874, loss = 0.16539653\n",
      "Epoch 19: w = 1.909, loss = 0.08633806\n",
      "Prediction after training: f(5) =  9.612\n"
     ]
    }
   ],
   "source": [
    "# Let's do this again, but replace the gradient calculations with autograd\n",
    "\n",
    "# f = w =* x\n",
    "# f = 2 * x (w=2)\n",
    "# We are taking the real value of W to be 2???\n",
    "\n",
    "# Training samples\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32) # Training input\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32) # 2 * x, Training output\n",
    "\n",
    "# w also has to be a tensor in this model\n",
    "# Since we are interested in the gradient of our loss with respect ot w, we need to specify\n",
    "# ... that w requires the gradient calculattionn\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad = True) # Initially 0\n",
    "\n",
    "# Model Prediciton\n",
    "def forward(x):\n",
    "    # Forward pass to follow the conventions of pytorch\n",
    "    return w * x\n",
    "\n",
    "# Loss = mean squared error\n",
    "def loss(y, y_predicted):\n",
    "    # y_predicted = model output\n",
    "    return (((y_predicted - y)**2).mean())\n",
    "\n",
    "# Remove manually computed gradient\n",
    "\n",
    "# Main program\n",
    "print(f'Prediction before training: f(5) =  {forward(5):.3f}') # f(5) should be 10\n",
    "\n",
    "# Training\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20 # Number of iteration\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediciton = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass (remember: back propogation)\n",
    "    l.backward() # Calculates dLoss/dWeights (dl/dw)\n",
    "    # PyTorch does all the calculations for us\n",
    "    \n",
    "    # Update weights\n",
    "    \"\"\"\n",
    "    When using PyTorch, we cannot have this operation in our gradient tracking graph\n",
    "        > Cannot be part of the computation graph\n",
    "        > We need to wrap it in torch.no_grad\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    # This is the update formula for gradient descent\n",
    "    # Our weight, W, goes in the negative direction of the learning rate * the gradient\n",
    "    \"\"\"\n",
    "    We also need to zero our gradients, because otherwise l.backward() will keep accumulating\n",
    "    the w.grad attribute\n",
    "    \"\"\"\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if(epoch % 2 == 0):\n",
    "        print(f'Epoch {epoch  + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "print(f'Prediction after training: f(5) =  {forward(5):.3f}')\n",
    "\"\"\"\n",
    "It ends up not as good as the one above becaus the back propogation is not as exact as the numerical\n",
    "gradient compuation\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
