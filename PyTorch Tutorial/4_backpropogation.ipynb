{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is convolution video\n",
    "# https://www.youtube.com/watch?v=KuXjwB4LzSA\n",
    "# This video is a masterpiece\n",
    "\n",
    "\"\"\"\n",
    "For those of you who have heard of a convolutional neural netowrk, the idea there i to use data\n",
    "to figure out what the kernels (the small array that gets flipped), as determined by whatever \n",
    "the neural netowrk wants to detect\n",
    "\"\"\"\n",
    "\n",
    "# The polynomial multiplication and FFT (Fast Fourier Transofrm) part are still totally confusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 13, 28, 27, 18])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.convolve((1,2,3), (4,5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The goal is to minimise the final output with respect to the initial ouput\n",
    "    > dx/dx = dx/dy * dy/dx\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "n mathematical optimization and decision theory, a loss function or cost function is a function that\n",
    "maps an event or values of one or more variables onto a real number intuitively representing some \n",
    "\"cost\" associated with the event. An optimization problem seeks to minimize a loss function\n",
    "\n",
    "dLoss/dx = dLoss/dz * dz/dx\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The whole concept consists of three steps:\n",
    "    1. Fowrard pass - apply all functions and compute loss\n",
    "    2. Compute local gradients (one at each node)\n",
    "    3. Backwards pass - compute dLoss/dWeights (gradient of loss with respect to weights / parameters)\n",
    "       using the chain rule\n",
    "       \n",
    "The real example makes this all make a bite more sense\n",
    "NOTE: We are only interested on the derivatives of the parameters that we want to update, NOT the \n",
    "fixed values\n",
    "\n",
    "The final goal of all of this is the gradient of the loss with respect to the weights\n",
    "    > dLoss/dWeights\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# Code version of the above mentioned example\n",
    "\n",
    "x = torch.tensor(1.0) # \".0\" to make these floats\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "weight = torch.tensor(1.0, requires_grad = True)\n",
    "# We are interested in the gradient of the wieght, so rg=T\n",
    "\n",
    "# Forward pass and compute the loss\n",
    "y_hat = weight * x\n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "print(loss) # tensor(1., grad_fn=<PowBackward0>)\n",
    "\n",
    "# Backwards pass\n",
    "# PyTorch will compute the local gradients automatically for us and also computes the backward pass\n",
    "# automatically for us\n",
    "loss.backward() # This is the whole gradient computation\n",
    "print(weight.grad) # This is the first gradient, after the first foward and backward pass, -2\n",
    "\n",
    "### Now we would update weights\n",
    "### and do next forward and backward pass\n",
    "### Do several iterations\n",
    "### Trying to minimise loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
